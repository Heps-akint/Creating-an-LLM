{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e3e03a-d1cc-4685-81b9-c150f640b955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device) #run first, if on cpu then dont run rest.\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "learning_rate = 3e-4 #learning rate can be experimented on to evaluate which value produces the best prformance and qaulity over time.\n",
    "max_iters = 100000\n",
    "eval_interval = 2500\n",
    "eval_iters = 250\n",
    "# dropout = 0.2 #this helps us train better by randomly taking 20% of the neurons out to prevent over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ed3257-944e-4e07-8a86-8860e64b457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "with open('plaintxt1.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    #print(txt[:100]) #printing first 300 characters\n",
    "\n",
    "chars = sorted(set(text)) #creates array of sorted characters from text\n",
    "print(chars)\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae20bfe-c03a-4d6d-8725-31da4a288b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,\n",
      "         0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,\n",
      "         0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47,\n",
      "        33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36,\n",
      "        25, 38, 28,  1, 39, 30,  1, 39, 50,  9])\n"
     ]
    }
   ],
   "source": [
    "#tokenizer code: can convert intergers to strigs and strings to intergers\n",
    "\n",
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join(int_to_string[i] for i in l)\n",
    "\n",
    "#example of endoder and decoder\n",
    "\n",
    "'''\n",
    "encode_hello = encode('hello')\n",
    "decode_hello = decode(encode_hello)\n",
    "print(encode_hello)\n",
    "print(decode_hello)\n",
    "'''\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:100]) #first 100 lines of encoded text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f63a16-0b65-4b8d-89ff-94e73849a6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[56, 54, 67,  1, 67, 68, 73,  1],\n",
      "        [57,  1, 72, 74, 55, 63, 58, 56],\n",
      "        [62, 67, 60,  1, 73, 61, 58,  1],\n",
      "        [62, 72,  1, 78, 68, 74, 71,  1]])\n",
      "targets:\n",
      "tensor([[54, 67,  1, 67, 68, 73,  1, 56],\n",
      "        [ 1, 72, 74, 55, 63, 58, 56, 73],\n",
      "        [67, 60,  1, 73, 61, 58,  1, 60],\n",
      "        [72,  1, 78, 68, 74, 71,  1, 66]])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data)) #the 0.8 represents 80% of the data\n",
    "training_data = data[:n]\n",
    "validation_data = data[n:]\n",
    "#The chunk of code above is for producing the training set and validation set split\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == 'train' else validation_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix) #This causes the long prints in the optimiser cell\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) #the .to(device) forces the batch onto the gpu or specified device\n",
    "    return x,y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "#print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf669d81-ebbb-40d1-bd18-dbd5c04c3337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input in tensor([1]) target is tensor(1)\n",
      "when input in tensor([1, 1]) target is tensor(28)\n",
      "when input in tensor([ 1,  1, 28]) target is tensor(39)\n",
      "when input in tensor([ 1,  1, 28, 39]) target is tensor(42)\n",
      "when input in tensor([ 1,  1, 28, 39, 42]) target is tensor(39)\n",
      "when input in tensor([ 1,  1, 28, 39, 42, 39]) target is tensor(44)\n",
      "when input in tensor([ 1,  1, 28, 39, 42, 39, 44]) target is tensor(32)\n",
      "when input in tensor([ 1,  1, 28, 39, 42, 39, 44, 32]) target is tensor(49)\n"
     ]
    }
   ],
   "source": [
    "#biagram with predictions and targets for training next character prediction\n",
    "#sequential with cpu's and parallel with gpu's\n",
    "\n",
    "x = training_data[:block_size]\n",
    "y = training_data[1:block_size+1]\n",
    "#this chunk of code shows what the current input is and then what the target would be\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('when input in', context, 'target is', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ada8b4a-b334-4856-8c25-5fd0747e2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #this is a decorator. It makes sure that Pytorch doesnt use gradient here. this improves performance, memory usage, computation etc...\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # this puts the model into evaliuation mode which allows it to be validated/evaluated at its optimal form and test it.\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() #this puts the model into training mode allowing it to update the weights anf biases.\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d2e886-0e57-4df1-b3f2-e359eba6f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cW:h Wiw(jR\n",
      "6b((qxe.G.k?ROQkfFdcVS,K4\"7PWij1s'cu-XZL\"(o_Ak8qhf3wUT3hzxh];:V,4)l eK4&G[8UNb\n",
      "Vc4Z lGs_JouFsez6(fiJl!V:*9ef2zMGou\"vs5pPl1gW,!df];n?RFit6ndt4]I4&X EN_JPwSC;e&3u\"LKw*qh0]OOnFVAO_2b'])MR(gd:*&kV&Hn*(XaZ[g!jMC)r*xz6T:FdKa8TCMu1]v6L2k*N3VX,b3HFfTMBdDjbX?ih5zzMyN3?b&hcPuI?g2PvOi\"Hv5LP zB],_Om\n",
      "gW2Hs?ADrz(jFDK,.Vdtsp4zx(NAB4?*_J\n",
      "FdEPB9fJPvG.7d!KIgTg!m\"dN3Pdi-JswuIyjC;mNEY,TgHLQvsZ08ruX_J&\"&yrv)8,[Eu44qasW,tK7V\n",
      "TSGX?g;eN5R((gjm:Fh9lyWmdob'n0Jib'KR0MNpr(C;yk_L)Eb9078P(r7[83izY6DBWijS1UO5]MC5F\n"
     ]
    }
   ],
   "source": [
    "#using an nn.module function inside an nn.module subclass, they are all learnable parameters\n",
    "\n",
    "#want to have a small learning rate (alpha) for your algorithmns so that oyu don't iterate is too large steps.\n",
    "\n",
    "#weight decay cuases any extreme parameter weights to decay so they dont have an overpowering influence on the performance\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #creating an embedding table. This is basically a look-up table. This is a grid with tokens where the probability for a predicted token can be seen.\n",
    "        \n",
    "#the forward pass function descripes how inputs into the network will be passed trough the layers.\n",
    "    #.view turns a matrix unpacked into x, y, z, etc... coordinates back into a tensor.\n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #B is for batch, T is for time, Channels is vocabulary size\n",
    "            logits = logits.view(B*T, C) #because we're paying attention to the vocabulary, or the Channles we can blent the batch and time. As long a logits and targets have the same batch and time then this will be fine.\n",
    "            #B and T are multiplied because Pytorch expects an input shape of B by C by etc... (e.g B, C, T) but the shape is B by T by C (B, T, C), so the B and T are combined into one.\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) #cross_entropy is a loss function\n",
    "\n",
    "        return logits, loss #logits are basically a bunch of floating point numbers which are normalised. They show the contribution of a single token to the whole embedding, basically a probability distribution for what you want to predict\n",
    "\n",
    "    def generate(self, index, max_new_tokens): #max_new_tokens indicates the max length of the generated text/tokens\n",
    "        # index is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #getting the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            #applying softmax funtion to get probability distribution.\n",
    "            probs = F.softmax(logits, dim=-1) #(B, C)\n",
    "            #sample from the probability distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n",
    "            #add the samples index to the running sequence of indices\n",
    "            index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "        return index #Remember to always end a funcion with this indentrd return\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device) #runs the model on specified device\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84621285-fab5-40e5-8c5b-34359b9c68dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, training loss: 4.8425, validation loss: 4.8540\n",
      "step: 250, training loss: 4.7974, validation loss: 4.7882\n",
      "step: 500, training loss: 4.7440, validation loss: 4.7396\n",
      "step: 750, training loss: 4.6853, validation loss: 4.6680\n",
      "step: 1000, training loss: 4.6163, validation loss: 4.6148\n",
      "step: 1250, training loss: 4.5663, validation loss: 4.5630\n",
      "step: 1500, training loss: 4.5154, validation loss: 4.4937\n",
      "step: 1750, training loss: 4.4419, validation loss: 4.4621\n",
      "step: 2000, training loss: 4.3950, validation loss: 4.4040\n",
      "step: 2250, training loss: 4.3398, validation loss: 4.3587\n",
      "step: 2500, training loss: 4.2863, validation loss: 4.2780\n",
      "step: 2750, training loss: 4.2486, validation loss: 4.2341\n",
      "step: 3000, training loss: 4.1956, validation loss: 4.2040\n",
      "step: 3250, training loss: 4.1437, validation loss: 4.1813\n",
      "step: 3500, training loss: 4.1166, validation loss: 4.1080\n",
      "step: 3750, training loss: 4.0413, validation loss: 4.0545\n",
      "step: 4000, training loss: 4.0130, validation loss: 4.0126\n",
      "step: 4250, training loss: 3.9553, validation loss: 3.9511\n",
      "step: 4500, training loss: 3.9099, validation loss: 3.9310\n",
      "step: 4750, training loss: 3.8551, validation loss: 3.9050\n",
      "step: 5000, training loss: 3.8209, validation loss: 3.8361\n",
      "step: 5250, training loss: 3.8131, validation loss: 3.8207\n",
      "step: 5500, training loss: 3.7788, validation loss: 3.7613\n",
      "step: 5750, training loss: 3.6942, validation loss: 3.7334\n",
      "step: 6000, training loss: 3.6880, validation loss: 3.7063\n",
      "step: 6250, training loss: 3.6471, validation loss: 3.6616\n",
      "step: 6500, training loss: 3.6050, validation loss: 3.6184\n",
      "step: 6750, training loss: 3.5622, validation loss: 3.5888\n",
      "step: 7000, training loss: 3.5427, validation loss: 3.5747\n",
      "step: 7250, training loss: 3.5057, validation loss: 3.5063\n",
      "step: 7500, training loss: 3.4641, validation loss: 3.4957\n",
      "step: 7750, training loss: 3.4334, validation loss: 3.4483\n",
      "step: 8000, training loss: 3.3982, validation loss: 3.4055\n",
      "step: 8250, training loss: 3.3820, validation loss: 3.3742\n",
      "step: 8500, training loss: 3.3575, validation loss: 3.3601\n",
      "step: 8750, training loss: 3.3213, validation loss: 3.3382\n",
      "step: 9000, training loss: 3.2890, validation loss: 3.3232\n",
      "step: 9250, training loss: 3.2771, validation loss: 3.2758\n",
      "step: 9500, training loss: 3.2395, validation loss: 3.2560\n",
      "step: 9750, training loss: 3.2244, validation loss: 3.2449\n",
      "step: 10000, training loss: 3.1788, validation loss: 3.1931\n",
      "step: 10250, training loss: 3.1788, validation loss: 3.1859\n",
      "step: 10500, training loss: 3.1496, validation loss: 3.1684\n",
      "step: 10750, training loss: 3.1441, validation loss: 3.1602\n",
      "step: 11000, training loss: 3.0923, validation loss: 3.1446\n",
      "step: 11250, training loss: 3.0660, validation loss: 3.0967\n",
      "step: 11500, training loss: 3.0462, validation loss: 3.0979\n",
      "step: 11750, training loss: 3.0343, validation loss: 3.0616\n",
      "step: 12000, training loss: 3.0258, validation loss: 3.0482\n",
      "step: 12250, training loss: 3.0133, validation loss: 3.0227\n",
      "step: 12500, training loss: 2.9923, validation loss: 2.9950\n",
      "step: 12750, training loss: 2.9496, validation loss: 3.0073\n",
      "step: 13000, training loss: 2.9593, validation loss: 2.9571\n",
      "step: 13250, training loss: 2.9301, validation loss: 2.9550\n",
      "step: 13500, training loss: 2.9183, validation loss: 2.9404\n",
      "step: 13750, training loss: 2.9050, validation loss: 2.9399\n",
      "step: 14000, training loss: 2.8857, validation loss: 2.9385\n",
      "step: 14250, training loss: 2.8637, validation loss: 2.9149\n",
      "step: 14500, training loss: 2.8679, validation loss: 2.8673\n",
      "step: 14750, training loss: 2.8450, validation loss: 2.8801\n",
      "step: 15000, training loss: 2.8182, validation loss: 2.8815\n",
      "step: 15250, training loss: 2.8209, validation loss: 2.8453\n",
      "step: 15500, training loss: 2.7981, validation loss: 2.8355\n",
      "step: 15750, training loss: 2.7953, validation loss: 2.8363\n",
      "step: 16000, training loss: 2.7973, validation loss: 2.8312\n",
      "step: 16250, training loss: 2.7841, validation loss: 2.8099\n",
      "step: 16500, training loss: 2.7675, validation loss: 2.7840\n",
      "step: 16750, training loss: 2.7453, validation loss: 2.7765\n",
      "step: 17000, training loss: 2.7329, validation loss: 2.7352\n",
      "step: 17250, training loss: 2.7481, validation loss: 2.7727\n",
      "step: 17500, training loss: 2.7376, validation loss: 2.7645\n",
      "step: 17750, training loss: 2.6970, validation loss: 2.7573\n",
      "step: 18000, training loss: 2.7063, validation loss: 2.7487\n",
      "step: 18250, training loss: 2.6891, validation loss: 2.7446\n",
      "step: 18500, training loss: 2.6976, validation loss: 2.7278\n",
      "step: 18750, training loss: 2.7184, validation loss: 2.7272\n",
      "step: 19000, training loss: 2.6700, validation loss: 2.7197\n",
      "step: 19250, training loss: 2.6789, validation loss: 2.7196\n",
      "step: 19500, training loss: 2.6387, validation loss: 2.6898\n",
      "step: 19750, training loss: 2.6544, validation loss: 2.6996\n",
      "step: 20000, training loss: 2.6520, validation loss: 2.7079\n",
      "step: 20250, training loss: 2.6177, validation loss: 2.6833\n",
      "step: 20500, training loss: 2.6380, validation loss: 2.6730\n",
      "step: 20750, training loss: 2.6182, validation loss: 2.6867\n",
      "step: 21000, training loss: 2.6262, validation loss: 2.6788\n",
      "step: 21250, training loss: 2.6200, validation loss: 2.6529\n",
      "step: 21500, training loss: 2.6097, validation loss: 2.6672\n",
      "step: 21750, training loss: 2.5817, validation loss: 2.6566\n",
      "step: 22000, training loss: 2.5906, validation loss: 2.6235\n",
      "step: 22250, training loss: 2.5968, validation loss: 2.6320\n",
      "step: 22500, training loss: 2.5884, validation loss: 2.6082\n",
      "step: 22750, training loss: 2.5932, validation loss: 2.6205\n",
      "step: 23000, training loss: 2.5919, validation loss: 2.6334\n",
      "step: 23250, training loss: 2.5639, validation loss: 2.6285\n",
      "step: 23500, training loss: 2.5631, validation loss: 2.6088\n",
      "step: 23750, training loss: 2.5521, validation loss: 2.6296\n",
      "step: 24000, training loss: 2.5724, validation loss: 2.6080\n",
      "step: 24250, training loss: 2.5559, validation loss: 2.6014\n",
      "step: 24500, training loss: 2.5898, validation loss: 2.6034\n",
      "step: 24750, training loss: 2.5460, validation loss: 2.6188\n",
      "step: 25000, training loss: 2.5724, validation loss: 2.6163\n",
      "step: 25250, training loss: 2.5493, validation loss: 2.5674\n",
      "step: 25500, training loss: 2.5362, validation loss: 2.6062\n",
      "step: 25750, training loss: 2.5628, validation loss: 2.5855\n",
      "step: 26000, training loss: 2.5200, validation loss: 2.5699\n",
      "step: 26250, training loss: 2.5454, validation loss: 2.5721\n",
      "step: 26500, training loss: 2.5216, validation loss: 2.5824\n",
      "step: 26750, training loss: 2.5407, validation loss: 2.5724\n",
      "step: 27000, training loss: 2.5183, validation loss: 2.5809\n",
      "step: 27250, training loss: 2.5458, validation loss: 2.5882\n",
      "step: 27500, training loss: 2.5375, validation loss: 2.5599\n",
      "step: 27750, training loss: 2.5311, validation loss: 2.5650\n",
      "step: 28000, training loss: 2.5183, validation loss: 2.5781\n",
      "step: 28250, training loss: 2.5575, validation loss: 2.5650\n",
      "step: 28500, training loss: 2.5343, validation loss: 2.5628\n",
      "step: 28750, training loss: 2.5033, validation loss: 2.5543\n",
      "step: 29000, training loss: 2.5171, validation loss: 2.5689\n",
      "step: 29250, training loss: 2.4982, validation loss: 2.5662\n",
      "step: 29500, training loss: 2.5163, validation loss: 2.5622\n",
      "step: 29750, training loss: 2.5156, validation loss: 2.5615\n",
      "step: 30000, training loss: 2.5065, validation loss: 2.5500\n",
      "step: 30250, training loss: 2.5124, validation loss: 2.5627\n",
      "step: 30500, training loss: 2.5132, validation loss: 2.5406\n",
      "step: 30750, training loss: 2.5321, validation loss: 2.5420\n",
      "step: 31000, training loss: 2.5125, validation loss: 2.5478\n",
      "step: 31250, training loss: 2.4794, validation loss: 2.5326\n",
      "step: 31500, training loss: 2.4950, validation loss: 2.5580\n",
      "step: 31750, training loss: 2.4768, validation loss: 2.5459\n",
      "step: 32000, training loss: 2.4948, validation loss: 2.5697\n",
      "step: 32250, training loss: 2.5025, validation loss: 2.5170\n",
      "step: 32500, training loss: 2.5275, validation loss: 2.5421\n",
      "step: 32750, training loss: 2.4907, validation loss: 2.5339\n",
      "step: 33000, training loss: 2.4711, validation loss: 2.5505\n",
      "step: 33250, training loss: 2.4933, validation loss: 2.5412\n",
      "step: 33500, training loss: 2.4791, validation loss: 2.5202\n",
      "step: 33750, training loss: 2.4783, validation loss: 2.5381\n",
      "step: 34000, training loss: 2.4974, validation loss: 2.5482\n",
      "step: 34250, training loss: 2.4698, validation loss: 2.5307\n",
      "step: 34500, training loss: 2.5089, validation loss: 2.5443\n",
      "step: 34750, training loss: 2.4661, validation loss: 2.5235\n",
      "step: 35000, training loss: 2.4987, validation loss: 2.5340\n",
      "step: 35250, training loss: 2.4751, validation loss: 2.5107\n",
      "step: 35500, training loss: 2.5013, validation loss: 2.5449\n",
      "step: 35750, training loss: 2.4763, validation loss: 2.5342\n",
      "step: 36000, training loss: 2.4551, validation loss: 2.5386\n",
      "step: 36250, training loss: 2.5009, validation loss: 2.5149\n",
      "step: 36500, training loss: 2.4537, validation loss: 2.5396\n",
      "step: 36750, training loss: 2.4887, validation loss: 2.5137\n",
      "step: 37000, training loss: 2.4805, validation loss: 2.5186\n",
      "step: 37250, training loss: 2.4720, validation loss: 2.5189\n",
      "step: 37500, training loss: 2.4702, validation loss: 2.4773\n",
      "step: 37750, training loss: 2.4907, validation loss: 2.5062\n",
      "step: 38000, training loss: 2.4692, validation loss: 2.5235\n",
      "step: 38250, training loss: 2.4710, validation loss: 2.4843\n",
      "step: 38500, training loss: 2.4770, validation loss: 2.5182\n",
      "step: 38750, training loss: 2.4717, validation loss: 2.5239\n",
      "step: 39000, training loss: 2.4762, validation loss: 2.5041\n",
      "step: 39250, training loss: 2.4887, validation loss: 2.5159\n",
      "step: 39500, training loss: 2.4837, validation loss: 2.5080\n",
      "step: 39750, training loss: 2.4594, validation loss: 2.4866\n",
      "step: 40000, training loss: 2.4617, validation loss: 2.5112\n",
      "step: 40250, training loss: 2.4852, validation loss: 2.5244\n",
      "step: 40500, training loss: 2.4537, validation loss: 2.5126\n",
      "step: 40750, training loss: 2.4466, validation loss: 2.5027\n",
      "step: 41000, training loss: 2.4660, validation loss: 2.4974\n",
      "step: 41250, training loss: 2.4771, validation loss: 2.5534\n",
      "step: 41500, training loss: 2.4850, validation loss: 2.5148\n",
      "step: 41750, training loss: 2.4682, validation loss: 2.5016\n",
      "step: 42000, training loss: 2.4654, validation loss: 2.5167\n",
      "step: 42250, training loss: 2.4371, validation loss: 2.5171\n",
      "step: 42500, training loss: 2.4559, validation loss: 2.5060\n",
      "step: 42750, training loss: 2.4559, validation loss: 2.4986\n",
      "step: 43000, training loss: 2.4895, validation loss: 2.4989\n",
      "step: 43250, training loss: 2.4557, validation loss: 2.5035\n",
      "step: 43500, training loss: 2.4463, validation loss: 2.5104\n",
      "step: 43750, training loss: 2.4577, validation loss: 2.4745\n",
      "step: 44000, training loss: 2.4667, validation loss: 2.4952\n",
      "step: 44250, training loss: 2.4540, validation loss: 2.4821\n",
      "step: 44500, training loss: 2.4649, validation loss: 2.4504\n",
      "step: 44750, training loss: 2.4769, validation loss: 2.5012\n",
      "step: 45000, training loss: 2.4581, validation loss: 2.4908\n",
      "step: 45250, training loss: 2.4711, validation loss: 2.5191\n",
      "step: 45500, training loss: 2.4566, validation loss: 2.4836\n",
      "step: 45750, training loss: 2.4421, validation loss: 2.4935\n",
      "step: 46000, training loss: 2.4906, validation loss: 2.5154\n",
      "step: 46250, training loss: 2.4538, validation loss: 2.5171\n",
      "step: 46500, training loss: 2.4426, validation loss: 2.5022\n",
      "step: 46750, training loss: 2.4462, validation loss: 2.4790\n",
      "step: 47000, training loss: 2.4517, validation loss: 2.4984\n",
      "step: 47250, training loss: 2.4292, validation loss: 2.5267\n",
      "step: 47500, training loss: 2.4473, validation loss: 2.4924\n",
      "step: 47750, training loss: 2.4572, validation loss: 2.5027\n",
      "step: 48000, training loss: 2.4527, validation loss: 2.5039\n",
      "step: 48250, training loss: 2.4555, validation loss: 2.4640\n",
      "step: 48500, training loss: 2.4459, validation loss: 2.5282\n",
      "step: 48750, training loss: 2.4524, validation loss: 2.5042\n",
      "step: 49000, training loss: 2.4718, validation loss: 2.5076\n",
      "step: 49250, training loss: 2.4695, validation loss: 2.4947\n",
      "step: 49500, training loss: 2.4219, validation loss: 2.5116\n",
      "step: 49750, training loss: 2.4326, validation loss: 2.4866\n",
      "step: 50000, training loss: 2.4307, validation loss: 2.4920\n",
      "step: 50250, training loss: 2.4419, validation loss: 2.5125\n",
      "step: 50500, training loss: 2.4719, validation loss: 2.4521\n",
      "step: 50750, training loss: 2.4474, validation loss: 2.4739\n",
      "step: 51000, training loss: 2.4500, validation loss: 2.4783\n",
      "step: 51250, training loss: 2.4612, validation loss: 2.4723\n",
      "step: 51500, training loss: 2.4448, validation loss: 2.5269\n",
      "step: 51750, training loss: 2.4411, validation loss: 2.4937\n",
      "step: 52000, training loss: 2.4668, validation loss: 2.4904\n",
      "step: 52250, training loss: 2.4365, validation loss: 2.5095\n",
      "step: 52500, training loss: 2.4494, validation loss: 2.5107\n",
      "step: 52750, training loss: 2.4638, validation loss: 2.4907\n",
      "step: 53000, training loss: 2.4631, validation loss: 2.5073\n",
      "step: 53250, training loss: 2.4233, validation loss: 2.4760\n",
      "step: 53500, training loss: 2.4350, validation loss: 2.4877\n",
      "step: 53750, training loss: 2.4101, validation loss: 2.4991\n",
      "step: 54000, training loss: 2.4450, validation loss: 2.4839\n",
      "step: 54250, training loss: 2.4440, validation loss: 2.4989\n",
      "step: 54500, training loss: 2.4366, validation loss: 2.4816\n",
      "step: 54750, training loss: 2.4300, validation loss: 2.4732\n",
      "step: 55000, training loss: 2.4581, validation loss: 2.5013\n",
      "step: 55250, training loss: 2.4326, validation loss: 2.4838\n",
      "step: 55500, training loss: 2.4437, validation loss: 2.4999\n",
      "step: 55750, training loss: 2.4341, validation loss: 2.5052\n",
      "step: 56000, training loss: 2.4259, validation loss: 2.5037\n",
      "step: 56250, training loss: 2.4512, validation loss: 2.4964\n",
      "step: 56500, training loss: 2.4457, validation loss: 2.4613\n",
      "step: 56750, training loss: 2.4258, validation loss: 2.4902\n",
      "step: 57000, training loss: 2.4526, validation loss: 2.5034\n",
      "step: 57250, training loss: 2.4453, validation loss: 2.4871\n",
      "step: 57500, training loss: 2.4279, validation loss: 2.4817\n",
      "step: 57750, training loss: 2.4300, validation loss: 2.4837\n",
      "step: 58000, training loss: 2.4362, validation loss: 2.4728\n",
      "step: 58250, training loss: 2.4282, validation loss: 2.4891\n",
      "step: 58500, training loss: 2.4255, validation loss: 2.4765\n",
      "step: 58750, training loss: 2.4157, validation loss: 2.4870\n",
      "step: 59000, training loss: 2.4780, validation loss: 2.4803\n",
      "step: 59250, training loss: 2.4184, validation loss: 2.5062\n",
      "step: 59500, training loss: 2.4373, validation loss: 2.4698\n",
      "step: 59750, training loss: 2.4036, validation loss: 2.4893\n",
      "step: 60000, training loss: 2.4376, validation loss: 2.4850\n",
      "step: 60250, training loss: 2.4593, validation loss: 2.4783\n",
      "step: 60500, training loss: 2.4337, validation loss: 2.4845\n",
      "step: 60750, training loss: 2.4453, validation loss: 2.4902\n",
      "step: 61000, training loss: 2.4290, validation loss: 2.4755\n",
      "step: 61250, training loss: 2.4397, validation loss: 2.4470\n",
      "step: 61500, training loss: 2.4459, validation loss: 2.4729\n",
      "step: 61750, training loss: 2.4130, validation loss: 2.5135\n",
      "step: 62000, training loss: 2.4332, validation loss: 2.4942\n",
      "step: 62250, training loss: 2.4220, validation loss: 2.4638\n",
      "step: 62500, training loss: 2.4521, validation loss: 2.4909\n",
      "step: 62750, training loss: 2.4325, validation loss: 2.4953\n",
      "step: 63000, training loss: 2.4331, validation loss: 2.4837\n",
      "step: 63250, training loss: 2.4319, validation loss: 2.4799\n",
      "step: 63500, training loss: 2.4388, validation loss: 2.5059\n",
      "step: 63750, training loss: 2.4405, validation loss: 2.4708\n",
      "step: 64000, training loss: 2.4486, validation loss: 2.5072\n",
      "step: 64250, training loss: 2.4398, validation loss: 2.4634\n",
      "step: 64500, training loss: 2.4267, validation loss: 2.5000\n",
      "step: 64750, training loss: 2.4432, validation loss: 2.4781\n",
      "step: 65000, training loss: 2.4629, validation loss: 2.4849\n",
      "step: 65250, training loss: 2.4386, validation loss: 2.4649\n",
      "step: 65500, training loss: 2.4440, validation loss: 2.4793\n",
      "step: 65750, training loss: 2.4405, validation loss: 2.5048\n",
      "step: 66000, training loss: 2.4309, validation loss: 2.4569\n",
      "step: 66250, training loss: 2.4568, validation loss: 2.5092\n",
      "step: 66500, training loss: 2.4502, validation loss: 2.4844\n",
      "step: 66750, training loss: 2.4308, validation loss: 2.4750\n",
      "step: 67000, training loss: 2.4189, validation loss: 2.4908\n",
      "step: 67250, training loss: 2.4345, validation loss: 2.5239\n",
      "step: 67500, training loss: 2.4381, validation loss: 2.4703\n",
      "step: 67750, training loss: 2.4369, validation loss: 2.4987\n",
      "step: 68000, training loss: 2.4404, validation loss: 2.4650\n",
      "step: 68250, training loss: 2.4522, validation loss: 2.4904\n",
      "step: 68500, training loss: 2.4602, validation loss: 2.4751\n",
      "step: 68750, training loss: 2.4499, validation loss: 2.4763\n",
      "step: 69000, training loss: 2.4267, validation loss: 2.4661\n",
      "step: 69250, training loss: 2.4346, validation loss: 2.4772\n",
      "step: 69500, training loss: 2.4386, validation loss: 2.4832\n",
      "step: 69750, training loss: 2.4404, validation loss: 2.5100\n",
      "step: 70000, training loss: 2.4380, validation loss: 2.4743\n",
      "step: 70250, training loss: 2.4107, validation loss: 2.4605\n",
      "step: 70500, training loss: 2.4209, validation loss: 2.4988\n",
      "step: 70750, training loss: 2.4398, validation loss: 2.4744\n",
      "step: 71000, training loss: 2.4222, validation loss: 2.5142\n",
      "step: 71250, training loss: 2.4353, validation loss: 2.4833\n",
      "step: 71500, training loss: 2.4164, validation loss: 2.4748\n",
      "step: 71750, training loss: 2.4341, validation loss: 2.4993\n",
      "step: 72000, training loss: 2.4025, validation loss: 2.4906\n",
      "step: 72250, training loss: 2.3994, validation loss: 2.4726\n",
      "step: 72500, training loss: 2.4312, validation loss: 2.4618\n",
      "step: 72750, training loss: 2.4369, validation loss: 2.4654\n",
      "step: 73000, training loss: 2.4518, validation loss: 2.4673\n",
      "step: 73250, training loss: 2.4139, validation loss: 2.4504\n",
      "step: 73500, training loss: 2.4262, validation loss: 2.4902\n",
      "step: 73750, training loss: 2.4292, validation loss: 2.4712\n",
      "step: 74000, training loss: 2.4069, validation loss: 2.5005\n",
      "step: 74250, training loss: 2.4406, validation loss: 2.4869\n",
      "step: 74500, training loss: 2.4169, validation loss: 2.4809\n",
      "step: 74750, training loss: 2.4570, validation loss: 2.4836\n",
      "step: 75000, training loss: 2.4317, validation loss: 2.5189\n",
      "step: 75250, training loss: 2.4343, validation loss: 2.4974\n",
      "step: 75500, training loss: 2.4324, validation loss: 2.4952\n",
      "step: 75750, training loss: 2.4270, validation loss: 2.5140\n",
      "step: 76000, training loss: 2.4131, validation loss: 2.4835\n",
      "step: 76250, training loss: 2.4371, validation loss: 2.4732\n",
      "step: 76500, training loss: 2.4411, validation loss: 2.4945\n",
      "step: 76750, training loss: 2.4415, validation loss: 2.4908\n",
      "step: 77000, training loss: 2.4039, validation loss: 2.4881\n",
      "step: 77250, training loss: 2.4396, validation loss: 2.4642\n",
      "step: 77500, training loss: 2.4162, validation loss: 2.4796\n",
      "step: 77750, training loss: 2.4395, validation loss: 2.4864\n",
      "step: 78000, training loss: 2.4618, validation loss: 2.4967\n",
      "step: 78250, training loss: 2.4404, validation loss: 2.4751\n",
      "step: 78500, training loss: 2.4148, validation loss: 2.4920\n",
      "step: 78750, training loss: 2.4364, validation loss: 2.5089\n",
      "step: 79000, training loss: 2.4232, validation loss: 2.4860\n",
      "step: 79250, training loss: 2.4330, validation loss: 2.4722\n",
      "step: 79500, training loss: 2.4360, validation loss: 2.4746\n",
      "step: 79750, training loss: 2.4256, validation loss: 2.4876\n",
      "step: 80000, training loss: 2.4392, validation loss: 2.4718\n",
      "step: 80250, training loss: 2.4376, validation loss: 2.4670\n",
      "step: 80500, training loss: 2.4185, validation loss: 2.4870\n",
      "step: 80750, training loss: 2.4707, validation loss: 2.5089\n",
      "step: 81000, training loss: 2.4465, validation loss: 2.4736\n",
      "step: 81250, training loss: 2.3980, validation loss: 2.4692\n",
      "step: 81500, training loss: 2.4394, validation loss: 2.4873\n",
      "step: 81750, training loss: 2.4208, validation loss: 2.4711\n",
      "step: 82000, training loss: 2.4410, validation loss: 2.4623\n",
      "step: 82250, training loss: 2.4085, validation loss: 2.4550\n",
      "step: 82500, training loss: 2.4447, validation loss: 2.5166\n",
      "step: 82750, training loss: 2.4356, validation loss: 2.4517\n",
      "step: 83000, training loss: 2.4283, validation loss: 2.4885\n",
      "step: 83250, training loss: 2.4195, validation loss: 2.4829\n",
      "step: 83500, training loss: 2.4279, validation loss: 2.4466\n",
      "step: 83750, training loss: 2.4023, validation loss: 2.4803\n",
      "step: 84000, training loss: 2.4124, validation loss: 2.4956\n",
      "step: 84250, training loss: 2.4403, validation loss: 2.5176\n",
      "step: 84500, training loss: 2.4437, validation loss: 2.4837\n",
      "step: 84750, training loss: 2.4728, validation loss: 2.4765\n",
      "step: 85000, training loss: 2.4088, validation loss: 2.4938\n",
      "step: 85250, training loss: 2.4548, validation loss: 2.4790\n",
      "step: 85500, training loss: 2.4202, validation loss: 2.4604\n",
      "step: 85750, training loss: 2.4181, validation loss: 2.4861\n",
      "step: 86000, training loss: 2.4375, validation loss: 2.4769\n",
      "step: 86250, training loss: 2.4545, validation loss: 2.4792\n",
      "step: 86500, training loss: 2.4241, validation loss: 2.4905\n",
      "step: 86750, training loss: 2.4098, validation loss: 2.4816\n",
      "step: 87000, training loss: 2.4291, validation loss: 2.4807\n",
      "step: 87250, training loss: 2.4210, validation loss: 2.4736\n",
      "step: 87500, training loss: 2.4357, validation loss: 2.4891\n",
      "step: 87750, training loss: 2.4188, validation loss: 2.4918\n",
      "step: 88000, training loss: 2.4202, validation loss: 2.4788\n",
      "step: 88250, training loss: 2.4243, validation loss: 2.4855\n",
      "step: 88500, training loss: 2.4409, validation loss: 2.4756\n",
      "step: 88750, training loss: 2.4293, validation loss: 2.4887\n",
      "step: 89000, training loss: 2.4231, validation loss: 2.5014\n",
      "step: 89250, training loss: 2.4394, validation loss: 2.4738\n",
      "step: 89500, training loss: 2.4233, validation loss: 2.4579\n",
      "step: 89750, training loss: 2.4412, validation loss: 2.4887\n",
      "step: 90000, training loss: 2.4560, validation loss: 2.4852\n",
      "step: 90250, training loss: 2.4338, validation loss: 2.4779\n",
      "step: 90500, training loss: 2.4207, validation loss: 2.4874\n",
      "step: 90750, training loss: 2.4104, validation loss: 2.4933\n",
      "step: 91000, training loss: 2.4351, validation loss: 2.4923\n",
      "step: 91250, training loss: 2.4144, validation loss: 2.4855\n",
      "step: 91500, training loss: 2.4267, validation loss: 2.4872\n",
      "step: 91750, training loss: 2.4292, validation loss: 2.4409\n",
      "step: 92000, training loss: 2.4458, validation loss: 2.4864\n",
      "step: 92250, training loss: 2.4167, validation loss: 2.4799\n",
      "step: 92500, training loss: 2.4329, validation loss: 2.4604\n",
      "step: 92750, training loss: 2.4214, validation loss: 2.4805\n",
      "step: 93000, training loss: 2.4254, validation loss: 2.4657\n",
      "step: 93250, training loss: 2.3926, validation loss: 2.4730\n",
      "step: 93500, training loss: 2.4286, validation loss: 2.4747\n",
      "step: 93750, training loss: 2.4662, validation loss: 2.4692\n",
      "step: 94000, training loss: 2.4360, validation loss: 2.4648\n",
      "step: 94250, training loss: 2.4215, validation loss: 2.4667\n",
      "step: 94500, training loss: 2.4200, validation loss: 2.4638\n",
      "step: 94750, training loss: 2.4138, validation loss: 2.4741\n",
      "step: 95000, training loss: 2.4156, validation loss: 2.4690\n",
      "step: 95250, training loss: 2.4066, validation loss: 2.4794\n",
      "step: 95500, training loss: 2.4183, validation loss: 2.4722\n",
      "step: 95750, training loss: 2.4300, validation loss: 2.4848\n",
      "step: 96000, training loss: 2.3994, validation loss: 2.4786\n",
      "step: 96250, training loss: 2.4109, validation loss: 2.4900\n",
      "step: 96500, training loss: 2.4259, validation loss: 2.4610\n",
      "step: 96750, training loss: 2.4164, validation loss: 2.4884\n",
      "step: 97000, training loss: 2.4453, validation loss: 2.5032\n",
      "step: 97250, training loss: 2.4188, validation loss: 2.4845\n",
      "step: 97500, training loss: 2.3986, validation loss: 2.4863\n",
      "step: 97750, training loss: 2.4137, validation loss: 2.4952\n",
      "step: 98000, training loss: 2.4025, validation loss: 2.4508\n",
      "step: 98250, training loss: 2.4317, validation loss: 2.4940\n",
      "step: 98500, training loss: 2.4061, validation loss: 2.4742\n",
      "step: 98750, training loss: 2.4097, validation loss: 2.4552\n",
      "step: 99000, training loss: 2.4285, validation loss: 2.4707\n",
      "step: 99250, training loss: 2.4198, validation loss: 2.4982\n",
      "step: 99500, training loss: 2.4271, validation loss: 2.4587\n",
      "step: 99750, training loss: 2.4273, validation loss: 2.4861\n",
      "2.1130006313323975\n",
      "CPU times: user 8min 50s, sys: 16.7 s, total: 9min 7s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating training loop using a Pytorch optimiser. This is the structure of a standard optimiser\n",
    "\n",
    "#where the learning takes place\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate) #Adam optimisation algorithm used for optimiser.\n",
    "\n",
    "#Getting an idea of hor the model is performing over time\n",
    "for iter in range(max_iters): #iters refers to the iterations, or no of training iterations\n",
    "    if iter % eval_iters == 0:\n",
    "        #reporting losses as the model trains\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, training loss: {losses['train']:.4f}, validation loss: {losses['val']:.4f}\") #Prints the iteration step and loss for itaerations divisible by eval_iters value\n",
    "    \n",
    "    #sampling batch of data xb is x batch and yb is y batch\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    #evaluate the loss using loss function\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    #optimising only with the current gradient of the current data\n",
    "    optimiser.zero_grad(set_to_none=True) #Pytorch accumulates and adds the gradients over time by default. Using .zero_grad makes stops the accumilation of preveous gradients from happening. This allows for previous sketchy gradients and data to not be influence current ones.\n",
    "    loss.backward() #This is a backward bass. Basically a reverse pass through\n",
    "    optimiser.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb5dd7d-fdd7-4b4a-81ad-ed60b506ab08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n***Optimises, what they do for us, and the differences/similarities between different optimisers.***\\n\\n1. **Mean Square Error (MSE)**: A common loss function used in regression problems (where the goal is to predict a continuous output). The function works by measuring the average squared difference between predicted and actual values and is often used to train neral nets for regression tasks.\\n2. **Gradient Descent (GD)**: Optimisation algorithm used to minimise the loss funtion of a machine learning model. The loss function measures how well the model is able to predict the target variable based on the input features. GD iteratively ajusts the model parameters in the direction of the steepest descent/decrease of the loss funtion.\\n3. **Momentum**: Momentum is an extension of GD that adds a momentum term to smooth out the trsining and keep it moving in teh right direction. Very useful for training neural nets.\\n4. **RMSprop**: RMSprop is an optimisation algorithm which uses moving averages that helps to adamt thr learning rate of algorithms. This helps to avoid oscillations, parameter updates and improve convergence.\\n5. **Adam**: Adam is an optimisation algorith which combines the ideas behind Momentum and RMSprop. It uses a moving average of both the gradient and is squared value to adapt the learning rate of each parameter. Used as a default algorithm for Deep Learning models.\\n6. **Adamw**: Modification of Adam optimisation algorithm which adds weigtht decay for parameters.\\n\\nfind more optimisers details at torch.optim\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "***Optimises, what they do for us, and the differences/similarities between different optimisers.***\n",
    "\n",
    "1. **Mean Square Error (MSE)**: A common loss function used in regression problems (where the goal is to predict a continuous output). The function works by measuring the average squared difference between predicted and actual values and is often used to train neral nets for regression tasks.\n",
    "2. **Gradient Descent (GD)**: Optimisation algorithm used to minimise the loss funtion of a machine learning model. The loss function measures how well the model is able to predict the target variable based on the input features. GD iteratively ajusts the model parameters in the direction of the steepest descent/decrease of the loss funtion.\n",
    "3. **Momentum**: Momentum is an extension of GD that adds a momentum term to smooth out the trsining and keep it moving in teh right direction. Very useful for training neural nets.\n",
    "4. **RMSprop**: RMSprop is an optimisation algorithm which uses moving averages that helps to adamt thr learning rate of algorithms. This helps to avoid oscillations, parameter updates and improve convergence.\n",
    "5. **Adam**: Adam is an optimisation algorith which combines the ideas behind Momentum and RMSprop. It uses a moving average of both the gradient and is squared value to adapt the learning rate of each parameter. Used as a default algorithm for Deep Learning models.\n",
    "6. **Adamw**: Modification of Adam optimisation algorithm which adds weigtht decay for parameters.\n",
    "\n",
    "find more optimisers details at torch.optim\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14cf97c6-2f62-4fd0-8827-a261f8f6fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dowouleth ild Yogs \"I wand n\n",
      "okegererason.\"Wik o ad. s t thingath rinxansemande y ine pllld. wantht sca\n",
      "\n",
      "cof Winin. we thin.  en On oupt angofr te agatoocress hind anstontidve oven ffsitlve mivestithe's suglo ge ers re f atellou s inge yosetheditoy Domehot cind ther, s OMantis waringe'll, owousel m s pleanche woof f e-he f teeldin sar tiled I m\n",
      "WOney,\"\n",
      "\"\n",
      "\n",
      "\n",
      "inin thomssaf od I'tth mave I's kitocorot, aitove w at y,\"There wan]7[E a h atrlaled re alas f thays cenn w atrgigge we an'se Pe upoma ly.\" \n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
